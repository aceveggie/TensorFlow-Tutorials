{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "  input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "  input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,274,762\n",
      "Trainable params: 3,274,698\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l = tf.keras.layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    l.Conv2D(\n",
    "        32, 5, padding='same', activation='relu', input_shape=input_shape),\n",
    "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    l.BatchNormalization(),\n",
    "    l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    l.Flatten(),\n",
    "    l.Dense(1024, activation='relu'),\n",
    "    l.Dropout(0.4),\n",
    "    l.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training logs to /tmp/tmp1uesqgd_\n"
     ]
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "print('Writing training logs to ' + logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6010\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f9a3160be80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0914 17:10:23.635754 140302187329344 deprecation.py:323] From /home/jason543/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.2335 - acc: 0.9459 - val_loss: 0.0791 - val_acc: 0.9882\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0467 - acc: 0.9850 - val_loss: 0.0301 - val_acc: 0.9910\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0327 - acc: 0.9900 - val_loss: 0.0224 - val_acc: 0.9925\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0252 - acc: 0.9922 - val_loss: 0.0273 - val_acc: 0.9907\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0216 - acc: 0.9927 - val_loss: 0.0304 - val_acc: 0.9905\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0188 - acc: 0.9938 - val_loss: 0.0307 - val_acc: 0.9918\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0236 - val_acc: 0.9936\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0149 - acc: 0.9954 - val_loss: 0.0383 - val_acc: 0.9890\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0139 - acc: 0.9956 - val_loss: 0.0369 - val_acc: 0.9927\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0132 - acc: 0.9958 - val_loss: 0.0408 - val_acc: 0.9905\n",
      "Test loss: 0.04082631714018103\n",
      "Test accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch=0)]\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to:  /tmp/tmpmav8qh34.h5\n"
     ]
    }
   ],
   "source": [
    "# Backend agnostic way to save/restore models\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "print('Saving model to: ', keras_file)\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity import keras as sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End step: 5628\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 12\n",
    "num_train_samples = x_train.shape[0]\n",
    "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
    "print('End step: ' + str(end_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 17:11:19.723565 140302187329344 deprecation.py:323] From /home/jason543/.local/lib/python3.6/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_conv2d_2 (None, 28, 28, 32)        1634      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d_3 (None, 14, 14, 64)        102466    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_2  (None, 1024)              6423554   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_3  (None, 10)                20492     \n",
      "=================================================================\n",
      "Total params: 6,548,274\n",
      "Trainable params: 3,274,698\n",
      "Non-trainable params: 3,273,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pruning_params = {\n",
    "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                   final_sparsity=0.90,\n",
    "                                                   begin_step=2000,\n",
    "                                                   end_step=end_step,\n",
    "                                                   frequency=100)\n",
    "}\n",
    "\n",
    "pruned_model = tf.keras.Sequential([\n",
    "    sparsity.prune_low_magnitude(\n",
    "        l.Conv2D(32, 5, padding='same', activation='relu'),\n",
    "        input_shape=input_shape,\n",
    "        **pruning_params),\n",
    "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    l.BatchNormalization(),\n",
    "    sparsity.prune_low_magnitude(\n",
    "        l.Conv2D(64, 5, padding='same', activation='relu'), **pruning_params),\n",
    "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    l.Flatten(),\n",
    "    sparsity.prune_low_magnitude(l.Dense(1024, activation='relu'),\n",
    "                                 **pruning_params),\n",
    "    l.Dropout(0.4),\n",
    "    sparsity.prune_low_magnitude(l.Dense(num_classes, activation='softmax'),\n",
    "                                 **pruning_params)\n",
    "])\n",
    "\n",
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training logs to /tmp/tmpbrbavyln\n"
     ]
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "print('Writing training logs to ' + logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6011\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f99fc149ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.1929 - acc: 0.9473 - val_loss: 0.1258 - val_acc: 0.9715\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0463 - acc: 0.9853 - val_loss: 0.0376 - val_acc: 0.9882\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0332 - acc: 0.9900 - val_loss: 0.0317 - val_acc: 0.9905\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0247 - acc: 0.9924 - val_loss: 0.0309 - val_acc: 0.9894\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0203 - acc: 0.9937 - val_loss: 0.0251 - val_acc: 0.9920\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0171 - acc: 0.9944 - val_loss: 0.0208 - val_acc: 0.9940\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0146 - acc: 0.9955 - val_loss: 0.0210 - val_acc: 0.9935\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.0144 - acc: 0.9956 - val_loss: 0.0217 - val_acc: 0.9926\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0150 - acc: 0.9949 - val_loss: 0.0219 - val_acc: 0.9927\n",
      "Epoch 10/10\n",
      "41600/60000 [===================>..........] - ETA: 0s - loss: 0.0118 - acc: 0.9963"
     ]
    }
   ],
   "source": [
    "pruned_model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
    "# step. Also add a callback to add pruning summaries to tensorboard\n",
    "callbacks = [\n",
    "    sparsity.UpdatePruningStep(),\n",
    "    sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
    "]\n",
    "\n",
    "pruned_model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = pruned_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,274,762\n",
      "Trainable params: 3,274,698\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model = sparsity.strip_pruning(pruned_model)\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving pruned model to:  /tmp/tmpsurllstt.h5\n"
     ]
    }
   ],
   "source": [
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "print('Saving pruned model to: ', pruned_keras_file)\n",
    "\n",
    "# No need to save the optimizer with the graph for serving.\n",
    "tf.keras.models.save_model(final_model, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the unpruned model before compression: 12.52 Mb\n",
      "Size of the unpruned model after compression: 11.59 Mb\n",
      "Size of the pruned model before compression: 12.52 Mb\n",
      "Size of the pruned model after compression: 2.51 Mb\n"
     ]
    }
   ],
   "source": [
    "_, zip1 = tempfile.mkstemp('.zip') \n",
    "with zipfile.ZipFile(zip1, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(keras_file)\n",
    "print(\"Size of the unpruned model before compression: %.2f Mb\" % \n",
    "      (os.path.getsize(keras_file) / float(2**20)))\n",
    "print(\"Size of the unpruned model after compression: %.2f Mb\" % \n",
    "      (os.path.getsize(zip1) / float(2**20)))\n",
    "\n",
    "_, zip2 = tempfile.mkstemp('.zip') \n",
    "with zipfile.ZipFile(zip2, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(pruned_keras_file)\n",
    "print(\"Size of the pruned model before compression: %.2f Mb\" % \n",
    "      (os.path.getsize(pruned_keras_file) / float(2**20)))\n",
    "print(\"Size of the pruned model after compression: %.2f Mb\" % \n",
    "      (os.path.getsize(zip2) / float(2**20)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 05:56:23.360528 140598366840640 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load the serialized model\n",
    "loaded_model = tf.keras.models.load_model(keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1876\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_conv2d ( (None, 28, 28, 32)        1634      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 14, 14, 32)        1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 14, 14, 32)        129       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d_1 (None, 14, 14, 64)        102466    \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 7, 7, 64)          1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_flatten  (None, 3136)              1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense (P (None, 1024)              6423554   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout  (None, 1024)              1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_1  (None, 10)                20492     \n",
      "=================================================================\n",
      "Total params: 6,548,279\n",
      "Trainable params: 3,274,698\n",
      "Non-trainable params: 3,273,581\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
    "print(end_step)\n",
    "\n",
    "new_pruning_params = {\n",
    "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                   final_sparsity=0.90,\n",
    "                                                   begin_step=0,\n",
    "                                                   end_step=end_step,\n",
    "                                                   frequency=100)\n",
    "}\n",
    "\n",
    "new_pruned_model = sparsity.prune_low_magnitude(model, **new_pruning_params)\n",
    "new_pruned_model.summary()\n",
    "\n",
    "new_pruned_model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training logs to /tmp/tmpbqph7o7e\n"
     ]
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "print('Writing training logs to ' + logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6009\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fde545a14a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0260 - val_acc: 0.9922\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0269 - val_acc: 0.9910\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0218 - acc: 0.9930 - val_loss: 0.0387 - val_acc: 0.9864\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0248 - val_acc: 0.9920\n",
      "Test loss: 0.024812044976648757\n",
      "Test accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
    "# step. Also add a callback to add pruning summaries to tensorboard\n",
    "callbacks = [\n",
    "    sparsity.UpdatePruningStep(),\n",
    "    sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
    "]\n",
    "\n",
    "new_pruned_model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = new_pruned_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,274,762\n",
      "Trainable params: 3,274,698\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model = sparsity.strip_pruning(pruned_model)\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving pruned model to:  /tmp/tmpwmu5myic.h5\n"
     ]
    }
   ],
   "source": [
    "_, new_pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "print('Saving pruned model to: ', new_pruned_keras_file)\n",
    "tf.keras.models.save_model(final_model, new_pruned_keras_file, \n",
    "                        include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the pruned model before compression: 12.52 Mb\n",
      "Size of the pruned model after compression: 2.51 Mb\n"
     ]
    }
   ],
   "source": [
    "_, zip3 = tempfile.mkstemp('.zip')\n",
    "with zipfile.ZipFile(zip3, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(new_pruned_keras_file)\n",
    "print(\"Size of the pruned model before compression: %.2f Mb\" \n",
    "      % (os.path.getsize(new_pruned_keras_file) / float(2**20)))\n",
    "print(\"Size of the pruned model after compression: %.2f Mb\" \n",
    "      % (os.path.getsize(zip3) / float(2**20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 05:59:05.527273 140598366840640 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "tflite_model_file = '/tmp/sparse_mnist.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_model_file, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tflite model before compression: 12.49 Mb\n",
      "Size of the tflite model after compression: 2.44 Mb\n"
     ]
    }
   ],
   "source": [
    "_, zip_tflite = tempfile.mkstemp('.zip')\n",
    "with zipfile.ZipFile(zip_tflite, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(tflite_model_file)\n",
    "print(\"Size of the tflite model before compression: %.2f Mb\" \n",
    "      % (os.path.getsize(tflite_model_file) / float(2**20)))\n",
    "print(\"Size of the tflite model after compression: %.2f Mb\" \n",
    "      % (os.path.getsize(zip_tflite) / float(2**20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 1000 images: 0.992000\n",
      "Accuracy after 2000 images: 0.990000\n",
      "Accuracy after 3000 images: 0.989333\n",
      "Accuracy after 4000 images: 0.989000\n",
      "Accuracy after 5000 images: 0.988800\n",
      "Accuracy after 6000 images: 0.990167\n",
      "Accuracy after 7000 images: 0.990714\n",
      "Accuracy after 8000 images: 0.991875\n",
      "Accuracy after 9000 images: 0.992444\n",
      "Accuracy after 10000 images: 0.992500\n",
      "0.9925\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "def eval_model(interpreter, x_test, y_test):\n",
    "  total_seen = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for img, label in zip(x_test, y_test):\n",
    "    inp = img.reshape((1, 28, 28, 1))\n",
    "    total_seen += 1\n",
    "    interpreter.set_tensor(input_index, inp)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_index)\n",
    "    if np.argmax(predictions) == np.argmax(label):\n",
    "      num_correct += 1\n",
    "\n",
    "    if total_seen % 1000 == 0:\n",
    "        print(\"Accuracy after %i images: %f\" %\n",
    "              (total_seen, float(num_correct) / float(total_seen)))\n",
    "\n",
    "  return float(num_correct) / float(total_seen)\n",
    "\n",
    "print(eval_model(interpreter, x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 06:00:57.382956 140598366840640 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "tflite_quant_model_file = '/tmp/sparse_mnist_quant.tflite'\n",
    "with open(tflite_quant_model_file, 'wb') as f:\n",
    "  f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tflite model before compression: 3.13 Mb\n",
      "Size of the tflite model after compression: 0.59 Mb\n"
     ]
    }
   ],
   "source": [
    "_, zip_tflite = tempfile.mkstemp('.zip')\n",
    "with zipfile.ZipFile(zip_tflite, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(tflite_quant_model_file)\n",
    "print(\"Size of the tflite model before compression: %.2f Mb\" \n",
    "      % (os.path.getsize(tflite_quant_model_file) / float(2**20)))\n",
    "print(\"Size of the tflite model after compression: %.2f Mb\" \n",
    "      % (os.path.getsize(zip_tflite) / float(2**20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 1000 images: 0.992000\n",
      "Accuracy after 2000 images: 0.990000\n",
      "Accuracy after 3000 images: 0.989333\n",
      "Accuracy after 4000 images: 0.989000\n",
      "Accuracy after 5000 images: 0.988800\n",
      "Accuracy after 6000 images: 0.990167\n",
      "Accuracy after 7000 images: 0.990714\n",
      "Accuracy after 8000 images: 0.991875\n",
      "Accuracy after 9000 images: 0.992444\n",
      "Accuracy after 10000 images: 0.992500\n",
      "0.9925\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_quant_model_file))\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "print(eval_model(interpreter, x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
